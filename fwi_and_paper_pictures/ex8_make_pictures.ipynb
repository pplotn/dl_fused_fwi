{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /home/plotnips/Dropbox/Log_extrapolation/scripts/DENISE-Black-Edition-master/for_pasha/env\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "import importlib\n",
    "import segyio\n",
    "from natsort import natsorted\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import utils.shared as sd\n",
    "import utils.loaders as ld\n",
    "import utils.vis as vis\n",
    "import utils.backbone as backbone\n",
    "\n",
    "# import shared as sd\n",
    "# import loaders as ld\n",
    "# import vis as vis\n",
    "\n",
    "\n",
    "# remove the sys.path... line and add pyapi_denise.py to the same directory with the notebook\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import pyapi_denise as api\n",
    "print(f'Python: {sys.prefix}')\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "fontsize = 11\n",
    "params = {\n",
    "    # 'text.latex.preamble': ['\\\\usepackage{gensymb}'],\n",
    "    # 'image.origin': 'lower',\n",
    "    'image.interpolation': 'nearest',\n",
    "    'figure.dpi' : 150,\n",
    "    # 'image.cmap': 'gray',\n",
    "    'axes.grid': False,\n",
    "    'savefig.dpi': 150,  # to adjust notebook inline plot size\n",
    "    'axes.labelsize':fontsize,  # fontsize for x and y labels (was 10)\n",
    "    'axes.titlesize':fontsize,\n",
    "    'font.size':fontsize,  # was 10\n",
    "    'legend.fontsize': fontsize,  # was 10\n",
    "    'xtick.labelsize':fontsize,\n",
    "    'ytick.labelsize':fontsize,\n",
    "    'text.usetex': True,\n",
    "    # 'figure.figsize': [3.39, 2.10],\n",
    "    'font.family': 'serif',\n",
    "}\n",
    "mpl.rcParams.update(params)\n",
    "# mpl.rcParams['figure.dpi']= 100\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISABLE_SAVEFIG = False\n",
    "root_pics = './pics/'\n",
    "par = copy.deepcopy(sd.par_default)\n",
    "\n",
    "# After sparsification by 8\n",
    "dt = 0.016 # s\n",
    "dx = 25.0 # m\n",
    "par_shot = {'dt': dt, 'dx': dx}\n",
    "dt_net = par_shot['dt']\n",
    "\n",
    "# Shortcuts\n",
    "max_freq = 15; _max_freq = 5; _pclip=0.1; _colors = ['k--', 'b']; \n",
    "_rule_low={'fhi': 3, 'btype' : 'low'}; _keys_exclude = ['raw', 'mid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Python 3 API for Denise-Black-Edition.\n",
      "Check binary in ../bin/denise\n",
      "Parse ../par/DENISE_marm_OBC.inp\n",
      "Current directory is /home/plotnips/Dropbox/Log_extrapolation/scripts/DENISE-Black-Edition-master/for_pasha\n",
      "Init paths at ./outputs/\n"
     ]
    }
   ],
   "source": [
    "denise_root = '../'\n",
    "d = api.Denise(denise_root, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory is /home/plotnips/Dropbox/Log_extrapolation/scripts/DENISE-Black-Edition-master/for_pasha\n",
      "Init paths at ./out_fwi_marm/\n"
     ]
    }
   ],
   "source": [
    "root_fwi = './out_fwi_marm/'\n",
    "d.save_folder = root_fwi\n",
    "d.set_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load obj from ./data/survey.pkl\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/survey.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-649c295aa4ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msurvey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/survey.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurvey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Log_extrapolation/scripts/DENISE-Black-Edition-master/for_pasha/utils/shared.py\u001b[0m in \u001b[0;36mload_obj\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;34m\"\"\"Same as above (need to check their identity before deleting this func)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Load obj from {fname}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/survey.pkl'"
     ]
    }
   ],
   "source": [
    "class Survey:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.src = None # object with sources\n",
    "        self.rec = None # object with receivers\n",
    "        self.vp = None # placeholder for velocity model\n",
    "        self.dx = None # grid spacing\n",
    "        self.wb_taper = None # Waterbottom placeholder\n",
    "        self.log_idx = None # location of log (grid nodes)\n",
    "        self.log_loc = None # location of log, meters\n",
    "        self.bpw = None # Source wavelet\n",
    "        self.dDT = None\n",
    "        self.dNT = None\n",
    "        \n",
    "        \n",
    "survey = sd.load_obj('./data/survey.pkl')\n",
    "\n",
    "src = survey.src\n",
    "rec = survey.rec\n",
    "vp = survey.vp\n",
    "dx = survey.dx\n",
    "wb_taper = survey.wb_taper\n",
    "log_idx = survey.log_idx\n",
    "bpw = survey.bpw\n",
    "log_loc = survey.log_loc\n",
    "dDT = survey.dDT\n",
    "dNT = survey.dNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = np.mean(src.x[1:] - src.x[:-1])\n",
    "print('Field data spacing between sources {:.2f} m'.format(dsrc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_baseline = vp.copy()\n",
    "print(vp.shape)\n",
    "\n",
    "marm_path = os.path.join(root_fwi, 'start/')\n",
    "os.makedirs(marm_path, exist_ok=True)\n",
    "print(marm_path)\n",
    "if not 'marmousi_II_marine.vp' in os.listdir(marm_path):\n",
    "    # Download Marmousi II model\n",
    "    os.system(f'wget https://github.com/daniel-koehn/DENISE-Benchmark/raw/master/Marmousi-II/start/marmousi_II_marine.vp -P {marm_path}')\n",
    "    os.system(f'wget https://github.com/daniel-koehn/DENISE-Benchmark/raw/master/Marmousi-II/start/marmousi_II_marine.vs -P {marm_path}')\n",
    "    os.system(f'wget https://github.com/daniel-koehn/DENISE-Benchmark/raw/master/Marmousi-II/start/marmousi_II_marine.rho -P {marm_path}')\n",
    "\n",
    "    # Download initial model for FWI\n",
    "    os.system(f'wget https://github.com/daniel-koehn/DENISE-Benchmark/raw/master/Marmousi-II/start/marmousi_II_start_1D.vp -P {marm_path}')\n",
    "    os.system(f'wget https://github.com/daniel-koehn/DENISE-Benchmark/raw/master/Marmousi-II/start/marmousi_II_start_1D.vs -P {marm_path}')\n",
    "    os.system(f'wget https://github.com/daniel-koehn/DENISE-Benchmark/raw/master/Marmousi-II/start/marmousi_II_start_1D.rho -P {marm_path}')\n",
    "\n",
    "def extend(x, ez, ex):\n",
    "    if ex > 0:\n",
    "        x = np.concatenate((x, np.flip(x[:, -ex:], -1)), 1)              # OX\n",
    "    if ez > 0:\n",
    "        x = np.concatenate((x, x.min() * np.ones((ez, x.shape[1]))), 0)  # OZ\n",
    "    return x\n",
    "\n",
    "def get_vp_vs_rho(vp):\n",
    "    vp = extend(vp, 15, 0)\n",
    "\n",
    "    print(f'Reshape {vp.shape} into {wb_taper.shape}...')\n",
    "\n",
    "    vp = resize(vp, wb_taper.shape, anti_aliasing=True)\n",
    "    vp = np.where(vp <= 1500.0, 1490.0, vp)\n",
    "    vp = extend(vp, 0, 264 + 10 * 8)\n",
    "\n",
    "    # shear velocity, [m/s]\n",
    "    vs = vp.copy() / (3 ** 0.5)\n",
    "    vs = np.where(vp < 1.01 * np.min(vp), 0, vs)\n",
    "\n",
    "    # density, [kg/m3] \n",
    "    rho = 1e3*0.3 * vp.copy()**0.25\n",
    "    rho = np.where(vp < 1.01 * np.min(vp), 1000, rho)\n",
    "    return vp, vs, rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_marm = ld.load_bin(f'{marm_path}marmousi_II_marine.vp', (500, 174))[:, 100:]\n",
    "print(vp_marm.min(), vp_marm.max())\n",
    "\n",
    "# This is what was used in generation of training data\n",
    "box_min = 1490.\n",
    "box_max = 4000. \n",
    "\n",
    "vmin_marm = vp_marm.min()\n",
    "vp_marm -= vmin_marm\n",
    "\n",
    "vmax_marm = vp_marm.max()\n",
    "vp_marm /= vmax_marm\n",
    "vp_marm = box_min  + vp_marm * (box_max - box_min)\n",
    "print(vp_marm.min(), vp_marm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp, vs, rho = get_vp_vs_rho(vp_marm)\n",
    "vis.plot_acquisition(vp, dx, src, rec, title='Vp')\n",
    "model = api.Model(vp, vs, rho, dx)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_marm_linear = ld.load_bin(f'{marm_path}marmousi_II_start_1D.vp', (500, 174))[:, 100:]\n",
    "vp_marm_linear -= vmin_marm\n",
    "vp_marm_linear /= vmax_marm\n",
    "vp_marm_linear = box_min  + vp_marm_linear * (box_max - box_min)\n",
    "\n",
    "vp_linear, vs_linear, rho_linear = get_vp_vs_rho(vp_marm_linear)\n",
    "vis.plot_acquisition(vp_linear, dx, src, rec, title='Vp')\n",
    "model_linear = api.Model(vp_linear, vs_linear, rho_linear, dx)\n",
    "\n",
    "log_linear = vp_linear[:, log_idx]\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set custom wavelet\n",
    "wls = bpw[:, ::2]\n",
    "src.wavelets = wls[:, :dNT]\n",
    "\n",
    "vis.plot_acquisition(vp, dx, src, rec, title='Vp')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model.vp[:, log_idx]\n",
    "# model_log = copy.deepcopy(model)\n",
    "\n",
    "log_dict = {'data':log/2, 'loc': log_loc}\n",
    "vis.plot_acquisition(vp[:, :500], dx, src, rec, log=log_dict)\n",
    "# vis.savefig('vinit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_taper = model.vp < 1.01 * model.vp.min()\n",
    "print(wb_taper.shape, model.vp.shape, vp_linear.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = 1490.\n",
    "vp_new = np.zeros_like(model.vp)\n",
    "vps=[]\n",
    "for i in range(wb_taper.shape[-1]):\n",
    "    wb = wb_taper.shape[0] - np.argmax(wb_taper[:,i]) + 2\n",
    "    fun = vmin * np.ones_like(vp_new[:, 0:1])\n",
    "    fun[wb:, :] = 420 + fun[wb:, :] + 12 * np.expand_dims(np.arange(len(fun[wb:, 0])), 1)\n",
    "    fun = fun[::-1, :]\n",
    "    vps.append(fun)\n",
    "vp_new = np.concatenate(vps, -1)\n",
    "ids, ide = -33, None\n",
    "vp_new[ids:ide, :] = vp_linear[ids:ide, :]\n",
    "# new_vp = gaussian_filter(new_vp.copy(), **sigma_truncate)\n",
    "\n",
    "print(vp_new.shape)\n",
    "log_new = vp_new[-len(log):, log_idx]\n",
    "\n",
    "fig, ax = plt.subplots(1,1); \n",
    "# ax_depth = np.arange(len(wlog)) * dx / 1000\n",
    "ax_depth = np.arange(len(log)) * dx / 1000\n",
    "# ax.plot(ax_depth, wlog[::-1] / 1000, 'b', label='CGG')\n",
    "ax.plot(ax_depth, log[::-1] / 1000, 'r', label='Well')\n",
    "ax.plot(ax_depth, log_linear[::-1] / 1000, 'k--', label='Init')\n",
    "ax.plot(ax_depth, log_new[::-1] / 1000, 'b', label='New init')\n",
    "ax.set_ylabel('Velocity, km/s')\n",
    "ax.set_xlabel('Depth, km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shear velocity, [m/s]\n",
    "vs_new = vp_new.copy() / (3 ** 0.5)\n",
    "vs_new = np.where(vp_new < 1.01 * np.min(vp_new), 0, vs_new)\n",
    "\n",
    "# density, [kg/m3] \n",
    "rho_new = 1e3*0.3 * vp_new.copy()**0.25\n",
    "rho_new = np.where(vp_new < 1.01 * np.min(vp_new), 1000, rho_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src, end='\\n\\n------------\\n')\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.verbose = 0    # don't show redundant print outs\n",
    "\n",
    "if d.DT is None:\n",
    "    d.DT = 0.002\n",
    "shots = d.get_shots(keys=['_p.'])\n",
    "\n",
    "par_shot = {'vmin': -0.05, 'vmax': 0.05}\n",
    "if shots:\n",
    "    print(f'Read {len(shots)} shots {shots[0].shape} into list')\n",
    "    for i in [int(np.floor(x)) for x in np.linspace(0, len(shots)-1, 2)]:\n",
    "        try:\n",
    "            shot_s = ld.divmax(shots[i])\n",
    "            vis.plot_shot(shot_s, pclip=0.1)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to fetch data for i={i}. {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = np.concatenate([np.expand_dims(s, 0) for s in shots], 0)\n",
    "print(data_p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.archs import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nothing(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dump = './trained_nets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = []\n",
    "titles = []\n",
    "colors = []\n",
    "fns = []\n",
    "\n",
    "n_ens = 10\n",
    "\n",
    "\n",
    "# UNet to predict low\n",
    "from ipynb.fs.defs.ex1_unet_l import UNet_ext\n",
    "nets.append(Ensemble(UNet_ext(1, 1), None, None, os.path.join(root_dump, 'ex1_unet_l/'), \n",
    "                     n_ens, old=True, single_out=True))\n",
    "titles.append('unet_l')\n",
    "colors.append('g')\n",
    "fns.append([nothing, nothing])\n",
    "\n",
    "\n",
    "# Multi-column to predict low (L)\n",
    "from ipynb.fs.defs.ex2_multi_l import Mixer\n",
    "nets.append(Ensemble(Mixer(1, 1), None, None, os.path.join(root_dump, 'ex2_multi_l/'), \n",
    "                     n_ens, old=True))\n",
    "titles.append('l')\n",
    "colors.append('navy')\n",
    "fns.append([nothing, nothing])\n",
    "\n",
    "\n",
    "# Multi-column to predict low + correlation loss (LC)\n",
    "from ipynb.fs.defs.ex4_multi_lc import Encoder, Head, HeadOld\n",
    "net_lp = Ensemble(Encoder(1), \n",
    "                     Head(layers=[(96, 32), (32, 16)], layers_out=(16, 1), kernel_sizes=[4, 4], strides=[2, 2], pads=[1, 1]), \n",
    "                     HeadOld(layers=[(96, 32), (32, 1)], kernel_sizes=[3, 3], strides=[1, 1]),\n",
    "                     os.path.join(root_dump, 'ex4_multi_lc/'), n_ens)\n",
    "nets.append(net_lp)\n",
    "titles.append('lc')\n",
    "colors.append('yellow')\n",
    "fns.append([nothing, nothing])\n",
    "\n",
    "\n",
    "# Predict low+corr+model, without ultra-low freqs in second channel of data decoder (LCM)\n",
    "net_lpm = Ensemble(Encoder(1), \n",
    "                     Head(layers=[(96, 32), (32, 16)], layers_out=(16, 1), kernel_sizes=[4, 4], strides=[2, 2], pads=[1, 1]), \n",
    "                     HeadOld(layers=[(96, 32), (32, 1)], kernel_sizes=[3, 3], strides=[1, 1]),\n",
    "                     os.path.join(root_dump, 'ex5_multi_lcm/'), n_ens)\n",
    "nets.append(net_lpm)\n",
    "titles.append('lcm')\n",
    "colors.append('r:')\n",
    "fns.append([nothing, nothing])\n",
    "\n",
    "nets = tuple(nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_blend(dat_lo, dat_hi, dt, fedge):\n",
    "    assert dat_lo.shape == dat_hi.shape\n",
    "    h, w = dat_lo.shape[-2:]\n",
    "    dat_lo_fx = np.fft.rfft(dat_lo, w)\n",
    "    dat_hi_fx = np.fft.rfft(dat_hi, w)\n",
    "    ff = np.fft.rfftfreq(dat_hi.shape[-1], d=dt)\n",
    "\n",
    "    where_left = np.where(ff < fedge)[0]\n",
    "    where_right = np.where(ff >= fedge)[0]\n",
    "    print(where_right.shape)\n",
    "    \n",
    "    dat_lo_fx[..., where_right] = dat_hi_fx[..., where_right]\n",
    "    out = np.fft.irfft(dat_lo_fx, w)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.002\n",
    "par_agc = {'win': 150, 'amp': 1, 'eps': 1e-6}\n",
    "par_mutter = {'k': 6, 'b': -100, 'r': 30}\n",
    "par_blend_low_high ={'fs': 1/dt, 'upscale': 8, 'pad': (0, 8), 'flo': 4, 'btype': 'high'}\n",
    "par_ref = {'fs': 1/0.002, 'flo': 4, 'fhi': 5, 'order': 8, 'btype': 'band'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_net(net, loader):\n",
    "    data_l = np.zeros_like(data_p)\n",
    "    data_h = np.zeros_like(data_p)\n",
    "    data_blend = np.zeros_like(data_p)\n",
    "\n",
    "    for i,f in enumerate(range(len(loader))):\n",
    "        clear_output()\n",
    "        print(f'{i+1}/{len(loader)}\\n{f}')\n",
    "        dh_cgg = loader.__getitem__(i)[0]\n",
    "\n",
    "        # Predict low-frequencies and upscale it to 2 ms sampling\n",
    "        lfp = signal.resample(net.predict(dh_cgg, pred_idx=0, pred_chan=1), 3000, axis=-1)\n",
    "\n",
    "        # Take input data (strict zero < 4 Hz) and upsample it to 2 ms sampling\n",
    "        hf = signal.resample(dh_cgg[0,...], 3000, axis=-1)\n",
    "        # And match it with respective range of field data\n",
    "        hf = sd.match_amp(hf, ld.bandpass(data_p[i, ...], **par_blend_low_high))\n",
    "\n",
    "        # Now extract range [4, 5] Hz and normalize low-freq prediction on it\n",
    "        should_be_this = np.abs(ld.bandpass(hf, **par_ref)).max()\n",
    "        got_this = np.abs(ld.bandpass(lfp, **par_ref)).max()  \n",
    "        ratio = should_be_this / got_this\n",
    "        lf = lfp * ratio\n",
    "        print(should_be_this, got_this, ratio)\n",
    "\n",
    "        data_l[i, ...] = lf \n",
    "        data_h[i, ...] = hf \n",
    "\n",
    "        hft = data_p[i, :, :]\n",
    "        blend = hard_blend(lf, hft, dt, 4)\n",
    "        blend = ld.mutter(blend, **par_mutter)\n",
    "        data_blend[i, ...] = blend\n",
    "    return data_blend, data_h, data_l \n",
    "\n",
    "# net = nets[-1]\n",
    "# data_blend, data_h, data_l = predict_for_net(net, loader_cgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson coefficients for each net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nets = nets[0:]\n",
    "selected_colors = colors[0:]\n",
    "selected_titles = titles[0:]\n",
    "print(selected_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_su_field = os.path.join(d._root_su, 'field/')\n",
    "\n",
    "root_pred = os.path.join('/'.join(root_su_field.split('/')[:-2]), '/field/')\n",
    "fnames = sd.get_fnames_pattern(root_su_field + '*.su.*')\n",
    "\n",
    "loader_cgg, cube_cgg, scalers = sd.make_cube_and_loader(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warining! The cell below takes about 10 minutes! It doesn't have much dependencies after that so no need to run it unless number of networks changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fhi = 3\n",
    "\n",
    "net_preds = []\n",
    "net_lbls = []\n",
    "for inet, this_net in enumerate(selected_nets):\n",
    "    data_blend, data_h, data_l = predict_for_net(this_net, loader_cgg)\n",
    "    these_preds = []\n",
    "    these_lbls = []\n",
    "    for idat in range(data_blend.shape[0]):\n",
    "        pred = data_blend[idat, ...]\n",
    "        lbl = data_p[idat, ...]\n",
    "        \n",
    "        pred = ld.bandpass(pred, fs=1/dt, fhi=this_fhi, btype='low', order=8)\n",
    "        lbl = ld.bandpass(lbl, fs=1/dt, fhi=this_fhi, btype='low', order=8)\n",
    "        \n",
    "        these_preds.append(np.expand_dims(pred, 0))\n",
    "        these_lbls.append(np.expand_dims(lbl, 0))\n",
    "        # This list contains [(1, noffset),...] pearson coefficients for each trace\n",
    "    net_preds.append(np.concatenate(these_preds, 0))\n",
    "    net_lbls.append(np.concatenate(these_lbls, 0))\n",
    "net_preds = tuple(net_preds)\n",
    "net_lbls = tuple(net_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(net_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson for offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsons_nets = []\n",
    "for inet in range(len(selected_nets)):\n",
    "    print(f'{inet+1}/{len(selected_nets)}', end='\\r')\n",
    "    this_pred = net_preds[inet]\n",
    "    this_lbl = net_lbls[inet]\n",
    "    pearsons_nets.append(np.concatenate([np.expand_dims(vis.pearsonr(this_lbl[i, ...], this_pred[i, ...])[:, 0], 0) for i in range(this_lbl.shape[0])]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_vs_offset(curves, ylabel='', val_true=1, ylim=None, legend_loc=3, plot_all=False):\n",
    "    mpl.rcParams['figure.dpi']= 200\n",
    "    mpl.rcParams['text.usetex'] = False\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "    ax.grid('on')\n",
    "    xax = np.arange(curves[0].shape[-1]) * dx / 1000\n",
    "    for icurve, curve in enumerate(curves):\n",
    "        mean_line = np.mean(curve, 0)# + icurve\n",
    "        std_line = np.std(curve, 0)\n",
    "        this_color, this_linestyle = vis.color_to_color_linestyle(selected_colors[icurve])\n",
    "        if plot_all:\n",
    "            ax.plot(np.repeat(np.expand_dims(xax, 0), curve.shape[0], 0).T, curve.T, this_color, alpha=0.05)\n",
    "        else:\n",
    "            ax.fill_between(xax, mean_line - std_line, mean_line + std_line, alpha=0.1, \n",
    "                            color=this_color, linestyle=this_linestyle)\n",
    "            ax.plot(xax, mean_line, color=this_color, linestyle=this_linestyle, label=titles[icurve])\n",
    "\n",
    "    ax.plot(xax, val_true * np.ones_like(mean_line), color='k', linestyle='--', label='True')\n",
    "    ax.set_xlabel('Offset, km')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    # plt.legend(loc=3, ncol=len(pearsons_nets) // 3)\n",
    "    if legend_loc:\n",
    "        plt.legend(loc=legend_loc, ncol=3)\n",
    "    mpl.rcParams['figure.dpi']= 100\n",
    "    mpl.rcParams['text.usetex'] = True\n",
    "    if ylim:\n",
    "        ax.set_ylim(ylim)\n",
    "\n",
    "plot_metric_vs_offset(pearsons_nets, ylabel='Pearson coefficient', val_true=1, ylim=[0, 1.1], \n",
    "                      plot_all=False, legend_loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each in its own square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "    \n",
    "def plot_metric_vs_offset(curves, ylabel='', val_true=1, ylim=None, legend_loc=3, plot_all=False):\n",
    "#     fig, ax = plt.subplots(1, len(curves), figsize=(2*len(curves), 2))\n",
    "    nx = len(curves)\n",
    "    nz = 1\n",
    "    fsize=2\n",
    "    plt.figure(figsize=(0.84*nx*fsize, nz*fsize))\n",
    "    gs1 = gridspec.GridSpec(nz, nx)\n",
    "    gs1.update(wspace=0.01, hspace=0.01)\n",
    "    \n",
    "    mean_unet = np.mean(curves[0], 0)\n",
    "    xax = np.arange(curves[0].shape[-1]) * dx / 1000\n",
    "    for icurve, curve in enumerate(curves):\n",
    "        ax = plt.subplot(gs1[icurve])\n",
    "        ax.grid('on')\n",
    "        mean_line = np.mean(curve, 0)# + icurve\n",
    "        std_line = np.std(curve, 0)\n",
    "        this_color, this_linestyle = vis.color_to_color_linestyle(selected_colors[icurve])\n",
    "\n",
    "        ax.fill_between(xax, mean_line - std_line, mean_line + std_line, alpha=0.1, \n",
    "                        color='navy', linestyle='solid')\n",
    "        if icurve > 0:\n",
    "            ax.plot(xax, mean_unet, color='black', linestyle='--')\n",
    "            ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.plot(xax, mean_line, color='navy', linestyle='solid')\n",
    "        ax.axis('auto')\n",
    "        ax.plot(xax, val_true * np.ones_like(mean_line), color='k', linestyle='--', label='True')\n",
    "        if ylim:\n",
    "            ax.set_ylim(ylim)\n",
    "\n",
    "plot_metric_vs_offset(pearsons_nets, ylabel='Pearson coefficient', val_true=1, ylim=[0, 1.1], \n",
    "                      plot_all=False, legend_loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter per net in accuracy axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_nets = []\n",
    "prs_nets = []\n",
    "for inet in range(len(selected_nets)):\n",
    "    this_pred = net_preds[inet]\n",
    "    this_lbl = net_lbls[inet]\n",
    "    rms_nets.append(np.mean(np.concatenate([np.expand_dims(np.array(vis.tracewise_rms(this_lbl[i, ...] - this_pred[i, ...]) / vis._handle_np_zeros(vis.tracewise_rms(this_lbl[i, ...]))), 0) for i in range(this_lbl.shape[0])])))   \n",
    "    prs_nets.append(np.mean(np.concatenate([np.expand_dims(np.array(vis.metric_pearson2(this_lbl[i, ...], this_pred[i, ...])), 0) for i in range(this_lbl.shape[0])])))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rms_nets[-1].min(), rms_nets[-1].max())\n",
    "print(min(rms_nets), max(rms_nets))\n",
    "print(min(prs_nets), max(prs_nets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for single net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nets[-1]\n",
    "data_blend, data_h, data_l = predict_for_net(net, loader_cgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hft = data_p[-1, ...]\n",
    "hf = data_h[-1, ...]\n",
    "lf = data_l[-1, ...]\n",
    "blend = data_blend[-1, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare shared range from 4 to 5 Hz in time-offset domain\n",
    "want_this = ld.bandpass(hf, **par_ref)\n",
    "have_this = ld.bandpass(lf, **par_ref)\n",
    "vis.plot_shot([want_this, have_this], pclip=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep analysis of what is below frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_bottom = 15\n",
    "print(crop_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Expected ideal low frequencies\n",
    "print(data_p.shape)\n",
    "\n",
    "raws = []\n",
    "agcs = []\n",
    "for this_fhi in [5, 4, 3]:\n",
    "    lft = ld.bandpass(data_p[i, :, :], fs=1/dt, fhi=this_fhi, btype='low', order=8)\n",
    "    lfs = ld.bandpass(data_blend[i, :, :], fs=1/dt, fhi=this_fhi, btype='low', order=8)\n",
    "\n",
    "    \n",
    "    lfta = vis.agc(ld.mutter(lft, **par_mutter), **par_agc)[0]\n",
    "    lfsa = vis.agc(ld.mutter(lfs, **par_mutter), **par_agc)[0]\n",
    "    lfta = ld.mutter(lfta, **par_mutter)[..., crop_bottom*4:-crop_bottom*8]\n",
    "    lfsa = ld.mutter(lfsa, **par_mutter)[..., crop_bottom*4:-crop_bottom*8]\n",
    "    \n",
    "    lft = lft[..., crop_bottom*4:-crop_bottom*8]\n",
    "    lfs = lfs[..., crop_bottom*4:-crop_bottom*8]\n",
    "    \n",
    "    print('Low true {}\\n\\tmin:\\t{:e}\\tmax:\\t{:e}'.format(lft.shape, lft.min(), lft.max()))\n",
    "    print('Low predicted {}\\n\\tmin:\\t{:e}\\tmax:\\t{:e}'.format(lfs.shape, lfs.min(), lfs.max()))\n",
    "\n",
    "    lfts = lft[..., ::8]\n",
    "    lfss = lfs[..., ::8]\n",
    "    lfts_lfss = lfts - lfss\n",
    "    \n",
    "    lftas = lfta[..., ::8]\n",
    "    lfsas = lfsa[..., ::8]\n",
    "    lftas_lfsas = lftas - lfsas\n",
    "    \n",
    "    raws.append([lfts, lfss, lfts_lfss])    \n",
    "    agcs.append([lftas, lfsas, lftas_lfsas])\n",
    "    \n",
    "    # lfs_match = match_amp(lfts, lfs)\n",
    "    vis.plot_shot([lfts, lfss, lfts - lfss], axis=False, pclip=0.125, title=f'Expectation / Reality {vis.relative_rms(lfts, lfts_lfss)}')\n",
    "    vis.plot_shot([lftas, lfsas, lftas - lfsas], axis=False, pclip=0.5, title=f'Expectation / Reality {vis.relative_rms(lftas, lftas_lfsas)}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis.plot_nxm(raws, pclip=0.01, colorbar=False, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis.plot_nxm(agcs, pclip=0.5, colorbar=False, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis.imgrid(agcs[-1][0], agcs[-1][0], [agcs[-1][1]], diff_of_diff=False, figsize=(9, 10), scatter_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean errors through entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_mean_true = []\n",
    "for_mean_pred = []\n",
    "\n",
    "this_fhi = 3\n",
    "for i in range(data_p.shape[0]):\n",
    "    for_mean_true.append(np.expand_dims(ld.bandpass(data_p[i, :, :], fs=1/dt, fhi=this_fhi, btype='low', order=8), 0))\n",
    "    for_mean_pred.append(np.expand_dims(ld.bandpass(data_blend[i, : :], fs=1/dt, fhi=this_fhi, btype='low', order=8), 0))\n",
    "for_mean_true = np.concatenate(for_mean_true, 0)\n",
    "for_mean_pred = np.concatenate(for_mean_pred, 0)\n",
    "print(for_mean_true.shape, for_mean_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_true = np.mean(for_mean_true, 0)[..., crop_bottom*4:-crop_bottom*8]\n",
    "mean_pred = np.mean(for_mean_pred, 0)[..., crop_bottom*4:-crop_bottom*8]\n",
    "std_true = np.std(for_mean_true, 0)[..., crop_bottom*8:-crop_bottom*8]\n",
    "std_pred = np.std(for_mean_pred, 0)[..., crop_bottom*8:-crop_bottom*8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(16, 4))\n",
    "vis.plot_shot([mean_true, mean_pred], pclip=0.0001, colorbar=False, ax=ax[0], axis='off', dpi=200)\n",
    "vis.plot_shot([std_true, std_pred], pclip=0.75, colorbar=False, ax=ax[1], dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rms = np.sqrt(np.sum((for_mean_true - for_mean_pred)**2)) / np.sqrt(np.sum((for_mean_true)**2))\n",
    "print(mean_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(16, 4))\n",
    "mean_true_a = vis.agc(mean_true, **par_agc)[0]\n",
    "mean_pred_a = vis.agc(mean_pred, **par_agc)[0]\n",
    "vis.plot_shot([mean_true_a, mean_pred_a, mean_true_a - mean_pred_a], pclip=0.9, colorbar=False, ax=ax[0], axis='off', dpi=200)\n",
    "vis.plot_shot([std_true, std_pred], pclip=0.75, colorbar=False, ax=ax[1], dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare data from true foldler and low folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_blend_ulow_low ={'fs': 1/dt, 'upscale': 8, 'pad': (0, 8), 'fhi': 3, 'btype': 'low', 'order': 8}\n",
    "\n",
    "tops, bottoms = [], []\n",
    "for i in range(0, data_p.shape[0], 10):\n",
    "    dtrue = data_p[i, ...]\n",
    "    dblend = data_blend[i, ...]\n",
    "    \n",
    "    # Bandpass and AGC of true data\n",
    "    tmp_true = ld.bandpass(dtrue, **par_blend_ulow_low)\n",
    "    tmp_true = vis.agc(tmp_true, **par_agc)[0][..., ::8]\n",
    "    tops.append(tmp_true[..., crop_bottom:-crop_bottom])\n",
    "    \n",
    "    # Bandpass and AGC of predicted data\n",
    "    tmp_blend = ld.bandpass(dblend, **par_blend_ulow_low)\n",
    "    tmp_blend = vis.agc(tmp_blend, **par_agc)[0][..., ::8]\n",
    "    bottoms.append(tmp_blend[..., crop_bottom:-crop_bottom])\n",
    "    \n",
    "vis.plot_nxm([tops, bottoms] ,figsize = (5* len(tops), 5), \n",
    "             colorbar=False, pclip=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = '/home/ovcharoo_bak/all_over'\n",
    "if not os.path.exists(root_data):\n",
    "    root_data = '/data/oleg/data/'\n",
    "    \n",
    "path_syn = os.path.join(root_data, 'test_syn_32_el_vel_marm/')\n",
    "jloader_tr, jloader_te, jloader_trr, jloader_ter = sd.init_loaders(\n",
    "                 root_src = path_syn,\n",
    "                 root_dst = os.path.join(root_data, 'paper_processed_marm_el/'))\n",
    "\n",
    "path_syn = os.path.join(root_data, 'test_syn_32_filt_el_vel/')\n",
    "jfloader_tr, jfloader_te, jfloader_trr, jfloader_ter = sd.init_loaders(\n",
    "                 root_src = path_syn,\n",
    "                 root_dst = os.path.join(root_data, 'paper_processed_marm_el/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_noise = os.path.join(root_data, 'test_cgg_data5')\n",
    "cube_hf = ld.make_noise_cube(os.path.join(path_noise, 'cube_dst_te_h.npy'))\n",
    "cube_mf = ld.make_noise_cube(os.path.join(path_noise, 'cube_dst_te_m.npy'))\n",
    "\n",
    "jloader_tr = ld.NoiseAdder(jloader_tr, cube_hf, cube_mf)\n",
    "jloader_te = ld.NoiseAdder(jloader_te, cube_hf, cube_mf)\n",
    "\n",
    "jfloader_tr = ld.NoiseAdder(jfloader_tr, cube_hf, cube_mf)\n",
    "jfloader_te = ld.NoiseAdder(jfloader_te, cube_hf, cube_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jjloader_tr = ld.JointLoader(jloader_tr, jloader_trr)\n",
    "# jjloader_te = ld.JointLoader(jloader_te, jloader_ter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(vis)\n",
    "d = jloader_te.__getitem__(0)\n",
    "# hs, ls, ms, us, mods, hf, lf, mf, uf, modf, hsr, lsr, msr, usr, _, hfr, lfr, mfr, ufr, _ = [v[0,...] for v in d]\n",
    "hs, ls, ms, us, mods, hf, lf, mf, uf, modf = [v[0,...] for v in d]\n",
    "\n",
    "print(len(d), hs.shape)\n",
    "no, nt = hs.shape[-2:]\n",
    "yax = np.arange(nt) * dt\n",
    "print(f\"src:\\t{hs.shape}\")\n",
    "titles = ['High', 'Low', 'Mid']\n",
    "ax = vis.plot_shot(np.concatenate([np.concatenate([hs, hf], 0),\n",
    "                                  np.concatenate([ls, lf], 0),\n",
    "                                  np.concatenate([us, uf], 0)], 1), \n",
    "                   colorbar=False, figsize=(2,6), \n",
    "                   pclip=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an ensemble\n",
    "net = nets[-1]\n",
    "curves_lr = []\n",
    "curves_data = []\n",
    "curves_encoder = []\n",
    "\n",
    "if isinstance(net, Blend):\n",
    "    net = net.ens[-1]\n",
    "    \n",
    "for subnet in net.nets:\n",
    "    curves_lr.append(subnet.running_metrics_lr)\n",
    "    curves_data.append(subnet.running_metrics_data)\n",
    "    curves_encoder.append(subnet.running_metrics_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rates\n",
    "print(list(curves_lr[0].keys()), list(curves_lr[0]['train'].keys()))\n",
    "n_epochs = len(curves_lr[0]['train']['lr_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weights\n",
    "print(list(curves_data[0].keys()), list(curves_data[0]['train'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "print(list(curves_encoder[0].keys()), list(curves_encoder[0]['train'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip_first = int(0.1 * n_epochs)\n",
    "skip_first = 0\n",
    "\n",
    "ctr_data, ctr_model, ctr_corr, cte_data, cte_model, cte_corr = [], [], [], [], [], []\n",
    "for curve in curves_encoder:\n",
    "    ctr_data.append(curve['train']['data'][skip_first:])\n",
    "    ctr_model.append(curve['train']['model'][skip_first:])\n",
    "    cte_data.append(curve['val']['data'][skip_first:])\n",
    "    cte_model.append(curve['val']['model'][skip_first:])\n",
    "    try:\n",
    "        ctr_corr.append(curve['train']['corr'][skip_first:])\n",
    "        cte_corr.append(curve['val']['corr'][skip_first:])\n",
    "    except:\n",
    "        print('Failed to get corr loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi']= 200\n",
    "def get_xax_min_max_mean(lst_data):\n",
    "#     print(f'Process list of {len(lst_data)}')\n",
    "    min_len = min([len(x) for x in lst_data])\n",
    "    dat = np.concatenate([np.array(x)[np.newaxis, :min_len] for x in lst_data], 0)\n",
    "#     cmin = np.min(dat, 0)\n",
    "#     cmax = np.max(dat, 0)\n",
    "    cstd = np.std(dat, 0)\n",
    "    cmean = np.mean(dat, 0)\n",
    "    cmax = cmean + cstd\n",
    "    cmin = cmean - cstd\n",
    "    xax = skip_first + np.arange(len(cmean))\n",
    "    return xax, cmin, cmax, cmean\n",
    "\n",
    "def plot_fillbetween(ax, lst_data, **kwargs):\n",
    "    xax, cmin, cmax, cmean = get_xax_min_max_mean(lst_data)\n",
    "    ax.fill_between(xax, cmin, cmax, alpha=0.125, **kwargs)\n",
    "    ax.plot(xax, cmean, **kwargs)\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "subaxis = [ax, ax.twinx()]\n",
    "plot_fillbetween(subaxis[0], ctr_model, color='b')\n",
    "plot_fillbetween(subaxis[0], cte_model, color='b', linestyle='--')\n",
    "plot_fillbetween(subaxis[1], ctr_data, color='r')\n",
    "plot_fillbetween(subaxis[1], cte_data, color='r', linestyle='--')\n",
    "try:\n",
    "    plot_fillbetween(subaxis[0], ctr_corr, color='lime')\n",
    "    plot_fillbetween(subaxis[0], cte_corr, color='lime', linestyle='--')\n",
    "except:\n",
    "    print('Failed to plot corr loss')\n",
    "\n",
    "subaxis[0].set_ylabel('Model fit')\n",
    "subaxis[1].set_ylabel('Data fit')\n",
    "subaxis[0].legend(['Model train', 'Model val', 'Corr train', 'Corr val'], loc=3)\n",
    "subaxis[1].legend(['Data train', 'Data val'], loc=1)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.grid()\n",
    "# ax.set_title('Loss curves')\n",
    "\n",
    "# vis.savefig('losses.png', disable=DISABLE_SAVEFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize to percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi']= 200\n",
    "\n",
    "mpl.rcParams.update({'legend.fontsize': 7})\n",
    "def get_xax_min_max_mean_relative(lst_data):\n",
    "    min_len = min([len(x) for x in lst_data])\n",
    "    dat = np.concatenate([np.array(x)[np.newaxis, :min_len] for x in lst_data], 0)\n",
    "    \n",
    "    # Standard deviation\n",
    "    cstd = np.std(dat, 0)\n",
    "    cstd_max, cstd_min = np.max(cstd), np.min(cstd)\n",
    "    \n",
    "    # Mean\n",
    "    cmean = np.mean(dat, 0)\n",
    "    cmean_max, cmean_min = np.max(cmean[:5]), np.min(cmean)\n",
    "    \n",
    "    # Std relative to mean\n",
    "    std_relative = cstd / cmean\n",
    "    \n",
    "    relative_drop = (cmean_max - cmean_min)/ cmean_max\n",
    "    cmean -= cmean_min\n",
    "    cmean /= cmean[:5].max()\n",
    "    cmean = 1 - cmean\n",
    "    cmean *=relative_drop\n",
    "    cmax = cmean + cstd / np.max(cstd) * std_relative\n",
    "    cmin = cmean - cstd / np.max(cstd) * std_relative\n",
    "    xax = skip_first + np.arange(len(cmean))\n",
    "    return xax, cmin, cmax, cmean\n",
    "\n",
    "def plot_fillbetween_relative(ax, lst_data, **kwargs):\n",
    "    xax, cmin, cmax, cmean = get_xax_min_max_mean_relative(lst_data)\n",
    "    ax.fill_between(xax, cmin, cmax, alpha=0.125, **kwargs)\n",
    "    ax.plot(xax, cmean, **kwargs)\n",
    "\n",
    "# Plotting starts here\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "plot_fillbetween_relative(ax, ctr_model, color='b')\n",
    "plot_fillbetween_relative(ax, cte_model, color='b', linestyle='--')\n",
    "plot_fillbetween_relative(ax, ctr_data, color='r')\n",
    "plot_fillbetween_relative(ax, cte_data, color='r', linestyle='--')\n",
    "try:\n",
    "    plot_fillbetween_relative(ax, ctr_corr, color='lime')\n",
    "    plot_fillbetween_relative(ax, cte_corr, color='lime', linestyle='--')\n",
    "except:\n",
    "    print('Failed to plot corr loss')\n",
    "\n",
    "ax.set_ylabel('Relative reduction')\n",
    "ax.legend(['Model train', 'Model val', 'Data train', 'Data val',  'Corr train', 'Corr val'], loc=1, ncol=3)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.grid()\n",
    "ax.invert_yaxis()\n",
    "# ax.set_ylim([0.4, -0.05])\n",
    "# ax.set_xlim([0, 80])\n",
    "# ax.set_title('Loss curves')\n",
    "\n",
    "# vis.savefig('losses.png', disable=DISABLE_SAVEFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_first = 0\n",
    "ctr_w1, ctr_w2, ctr_w3, ctr_w1w2w3 = [], [], [], []\n",
    "for curve in curves_data:\n",
    "    ctr_w1.append(curve['train']['w1'][skip_first:])\n",
    "    ctr_w2.append(curve['train']['w2'][skip_first:])\n",
    "    ctr_w3.append(curve['train']['w3'][skip_first:])\n",
    "    ctr_w1w2w3.append(curve['train']['w_s1s2s3'][skip_first:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_nested_list(lst, k):\n",
    "    return [[k * v for v in l] for l in lst]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "plot_fillbetween(ax, ctr_w1, color='r')\n",
    "plot_fillbetween(ax, scale_nested_list(ctr_w2, 1), color='b')\n",
    "plot_fillbetween(ax, scale_nested_list(ctr_w3, 1), color='lime')\n",
    "plot_fillbetween(ax, ctr_w1w2w3, color='y')\n",
    "ax.set_ylabel('Loss weights')\n",
    "ax.legend([r'$1/2\\sigma_d$', \n",
    "           r'$1/2\\sigma_m$', \n",
    "           r'$1/2\\sigma_c$', \n",
    "           r'$log(\\sigma_d\\sigma_m\\sigma_c)$'], loc=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.grid()\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_title()\n",
    "\n",
    "# vis.savefig('sigmas.png', disable=DISABLE_SAVEFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve = curves_lr[0]['train']['lr_data']\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "ax.plot(np.arange(len(curve)), curve, color='r')\n",
    "ax.set_ylabel('Learning rate')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.grid()\n",
    "\n",
    "vis.savefig('lr.png', disable=DISABLE_SAVEFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['text.usetex'] = False\n",
    "mpl.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match shot amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_amp(have, want):\n",
    "    max_have = np.max(have[20:50,:-50])\n",
    "    max_want = np.max(want[20:50,:-50])\n",
    "    return have / max_have * max_want\n",
    "\n",
    "# def match_amp(have, want):\n",
    "#     max_have = np.max(have[20:50,20:-50])\n",
    "#     max_want = np.max(want[20:50,20:-50])\n",
    "#     return have / max_have * max_want\n",
    "\n",
    "d = jfloader_te.__getitem__(0)\n",
    "hst, lst, mst, ust, modst, hft, lft, mft, uft, modft = [v[0,...] for v in d]\n",
    "_lst = lst.copy()\n",
    "_lst[20:50, 20:-50] = -1\n",
    "vis.plot_shot(_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_shot([hst[..., ], lst, ust], pclip=0.25, colorbar=False, dt=dt_net, dx=1, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.plot_model(np.flip(modst, -1).T, colorbar=False, axis=False, dpi=200)\n",
    "pclip=0.25; vis.plot_shot([hst, lst, ust, modst*pclip], pclip=pclip, figsize=(12, 4), axis=False, colorbar=False, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(selected_titles) == len(nets), 'There are more names than nets! CHECK!'\n",
    "print(len(net_lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import metrics\n",
    "\n",
    "def r2(lbl, pred, dim):\n",
    "    top = np.sum((lbl - pred) ** 2, axis=-1, keepdims=True)\n",
    "    bottom = np.sum((lbl - np.mean(lbl, axis=-1, keepdims=True)) ** 2, -1, keepdims=True)\n",
    "    return 1 - (top / bottom)\n",
    "\n",
    "def compound_eval(ref, pred, threshold=0.5):\n",
    "    tracewise_pearson = vis.pearsonr(ref, pred, -1)\n",
    "    num_total = np.prod(ref.shape[:-1])\n",
    "    num_good = len(tracewise_pearson[tracewise_pearson > threshold])\n",
    "    return num_good / num_total\n",
    "\n",
    "# def struct_similarity(lbl, pred):\n",
    "#     win = int(0.2 * lbl.shape[-2])\n",
    "#     print(win)\n",
    "#     if win % 2 == 0:\n",
    "#         win += 1\n",
    "#     return metrics.structural_similarity(lbl, pred, gaussian_weights=False, win_size=win)\n",
    "\n",
    "def struct_similarity(lbl, pred):\n",
    "    win = int(0.1 * lbl.shape[-2])\n",
    "    if win % 2 == 0:\n",
    "        win += 1\n",
    "    ssims = []\n",
    "    for ip in range(pred.shape[0]):\n",
    "        ssims.append(metrics.structural_similarity(lbl[ip,...], pred[ip,...], gaussian_weights=False, win_size=win))\n",
    "    return np.array(ssims)\n",
    "\n",
    "\n",
    "def print_stats(lbls, preds, titles):\n",
    "    dct = {'title': [], \n",
    "           'r2': [],\n",
    "           'ssim': [],\n",
    "           'pear': [], \n",
    "           'q50': [],\n",
    "          }\n",
    "    \n",
    "    print('Calculating Mean. Wait', end='')\n",
    "    for title, lbl, pred in zip(titles, lbls, preds):\n",
    "#         print(f'> {title}\\t{pred.shape}->{lbl.shape}...')\n",
    "        print('.', end='')\n",
    "        dct['title'].append(title)\n",
    "        # Mean\n",
    "        dct['r2'].append(np.mean(r2(lbl, pred, -1)))\n",
    "        dct['ssim'].append(np.mean(struct_similarity(lbl, pred)))\n",
    "        dct['pear'].append(np.mean(vis.pearsonr(lbl, pred, -1)))\n",
    "        dct['q50'].append(compound_eval(lbl, pred, threshold=0.5))\n",
    "   \n",
    "    df = pd.DataFrame.from_dict(dct, )\n",
    "    tmp = df.select_dtypes(include=[np.number])\n",
    "    df.loc[:, tmp.columns] = np.round(tmp, 2)\n",
    "    print('\\n')\n",
    "    print(df)\n",
    "    print('\\n')\n",
    "    \n",
    "    dct['r2'] = []\n",
    "    dct['ssim'] = []\n",
    "    dct['pear'] = []\n",
    "    print('Calculating STD. Wait', end='')\n",
    "    for title, lbl, pred in zip(titles, lbls, preds):\n",
    "#         print(f'> {title}\\t{pred.shape}->{lbl.shape}...')\n",
    "        print('.', end='')\n",
    "        # STD\n",
    "        dct['r2'].append(np.std(r2(lbl, pred, -1)))\n",
    "        dct['ssim'].append(np.std(struct_similarity(lbl, pred)))\n",
    "        dct['pear'].append(np.std(vis.pearsonr(lbl, pred, -1)))\n",
    "   \n",
    "    df = pd.DataFrame.from_dict(dct, )\n",
    "    tmp = df.select_dtypes(include=[np.number])\n",
    "    df.loc[:, tmp.columns] = np.round(tmp, 2)\n",
    "    print('\\n')\n",
    "    print(df)\n",
    "    \n",
    "\n",
    "crop_t = 10\n",
    "_net_lbls = [p[..., ::8] for p in net_lbls]\n",
    "_net_preds = [p[..., ::8] for p in net_preds]\n",
    "_net_preds = [match_amp(p, l) for p, l in zip(_net_preds, _net_lbls)]\n",
    "\n",
    "print_stats([p[..., crop_t:-crop_t] for p in _net_lbls], \n",
    "            [p[..., crop_t:-crop_t] for p in _net_preds], \n",
    "            selected_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_title_score(tts, scrs):\n",
    "    print('\\n'.join([str(t).ljust(10) + f'\\t{v}' for (t, v) in zip(tts, scrs)]))\n",
    "\n",
    "    \n",
    "# BANDPASS\n",
    "par_bp = copy.copy(par['rules']['ulow']); \n",
    "par_bp['fhi'] = 3\n",
    "par_bp['fs'] = 1/dt_net;\n",
    "par_bp['upscale'] = 8\n",
    "par_bp['pad'] = (0, 8)\n",
    "\n",
    "# par_bp['flo'] = 0.5\n",
    "# par_bp['btype'] = 'band'\n",
    "\n",
    "print(par_bp)\n",
    "\n",
    "vlim_low = {'vmin': -0.1, 'vmax': 0.1}\n",
    "vlim_ulow = {'vmin': -0.005, 'vmax': 0.005}\n",
    "\n",
    "imid = 1\n",
    "\n",
    "def insert_true_to_preds(index, channel, preds, fns, modst, lft, uft, titles):\n",
    "    if index == 1:\n",
    "        preds.insert(0, fns[0][0](modst)[::4,::4])\n",
    "    else:\n",
    "        if channel == 1:\n",
    "            print('Insert low')\n",
    "            preds.insert(0, fns[0][0](lft))\n",
    "        else:\n",
    "            print('Insert ultra-low')\n",
    "            preds.insert(0, fns[0][0](uft))\n",
    "    if 'True' not in titles:\n",
    "        titles.insert(0, 'True')\n",
    "        colors.insert(0, 'k--')\n",
    "    return preds, titles, colors\n",
    "\n",
    "def img_for_all_nets(nets, titles, colors, loader, imid, index, channel):\n",
    "    d = loader.__getitem__(imid)\n",
    "    hst, lst, mst, ust, modst, hft, lft, mft, uft, modft = [v[0,...] for v in d]\n",
    "\n",
    "    preds_raw = [fns[inet][-1](net.predict(fns[inet][0](hft), index, channel, return_type='list')) for inet, net in enumerate(nets)]\n",
    "    preds_raw = [[resize(p, (324, 376)) for p in sublist] for sublist in preds_raw]\n",
    "    print('Raw prediction list length (should be all the same:\\n\\t{})'.format([len(p) for p in preds_raw]))\n",
    "    preds_list = [[np.expand_dims(p, 0) for p in sublist] for sublist in preds_raw]\n",
    "    preds_list_bp = [[np.expand_dims(ld.bandpass(p, **par_bp), 0) for p in sublist] for sublist in preds_raw]\n",
    "\n",
    "    # Mean prediction\n",
    "    preds = [np.mean(np.concatenate(p, 0), 0) for p in preds_list]\n",
    "    preds_bp = [np.mean(np.concatenate(p, 0), 0) for p in preds_list_bp]\n",
    "    # Std of predictions\n",
    "    preds_std = [np.std(np.concatenate(p, 0), 0) for p in preds_list]\n",
    "    preds_std_bp = [np.std(np.concatenate(p, 0), 0) for p in preds_list_bp]\n",
    "    \n",
    "    par_table_field = vlim_low if index == 0 else {'vmin': None, 'vmax': None}\n",
    "    par_table_field2 = vlim_ulow if index == 0 else {'vmin': None, 'vmax': None}\n",
    "\n",
    "    if len(preds) == len(nets):\n",
    "        preds, titles, colors = insert_true_to_preds(index, channel, preds, fns, modst, lft, uft, titles)\n",
    "        preds_bp.insert(0, ld.bandpass(preds[0], **par_bp))\n",
    "        preds_std.insert(0, np.zeros_like(lft))\n",
    "        preds_std_bp.insert(0, np.zeros_like(lft))\n",
    "\n",
    "    preds = tuple(preds)\n",
    "    preds_bp = tuple(preds_bp)\n",
    "\n",
    "    preds = [p[..., 10:-10] for p in preds]\n",
    "    preds_bp = [p[..., 10:-10] for p in preds_bp]\n",
    "    preds_std_bp = [p[..., 10:-10] for p in preds_std_bp]\n",
    "    \n",
    "    # PLOT\n",
    "    diffs = [preds[0] - p for p in copy.copy(preds)];    \n",
    "    diffs = [preds_bp[0] - p for p in copy.copy(preds_bp)];# print([vis.relative_rms(preds[0], p) for p in diffs])\n",
    "   \n",
    "    # Row-plots\n",
    "    vis.plot_nxm([preds], figsize=(2*len(preds_bp), 2),  colorbar=False, **par_table_field)\n",
    "    vis.plot_nxm([preds_bp], figsize=(2*len(preds_bp), 2),  colorbar=False, **par_table_field2)\n",
    "    print(preds[0].shape, preds_bp[0].shape)\n",
    "    \n",
    "    # STD\n",
    "    vis.plot_nxm([preds_std], figsize=(2*len(preds_std), 2),  colorbar=False, cmap='magma', vmin=0, pclip=0.25)\n",
    "    vis.plot_nxm([preds_std_bp], figsize=(2*len(preds_std_bp), 2),  colorbar=False, cmap='magma', vmin=0, pclip=0.25)\n",
    "    \n",
    "    # =================\n",
    "    # COMPARE SELECTED    \n",
    "    # =================\n",
    "    cid1, cid2 = 3, -1\n",
    "    print(titles[cid1], titles[cid2])\n",
    "    vis.imgrid(match_amp(preds[0], hft), preds[0], (preds[cid1], preds[cid2]), \n",
    "               diff_of_diff=True, scatter_size=4, figsize=(6, 3.5), **par_table_field)\n",
    "    \n",
    "    vis.imgrid(match_amp(preds_bp[0], hft), preds_bp[0], (preds_bp[cid1], preds_bp[cid2]), \n",
    "               diff_of_diff=True, scatter_size=4, figsize=(6, 3.5), **par_table_field2)\n",
    "    \n",
    "    print(titles)\n",
    "    return preds, preds_bp, titles, colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi']= 150\n",
    "preds, preds_bp, titles, colors = img_for_all_nets(nets, titles, colors, jloader_te, imid, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 3x2: input, low, ulow + 3 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def make_pic_3x2(nets, inet, title, loader, imid):\n",
    "    d = loader.__getitem__(imid)\n",
    "    hst, lst, mst, ust, modst, hft, lft, mft, uft, modft = [v[0,...] for v in d]\n",
    "\n",
    "    preds = [fns[inet][-1](nets[inet].predict(fns[inet][0](hft), index, channel)) \n",
    "             for index, channel in [(0,0), (0,1), (1,0)]]\n",
    "    up, lp, mp = preds\n",
    "\n",
    "    lp_bp = ld.bandpass(lp, **par_bp)\n",
    "    \n",
    "    fsize = 2\n",
    "    nz, nx = 3, 2\n",
    "    plt.figure(figsize=(0.84*nx*fsize, nz*fsize))\n",
    "    gs1 = gridspec.GridSpec(nz, nx)\n",
    "    gs1.update(wspace=0.01, hspace=0.01) # set the spacing between axes. \n",
    "\n",
    "#     fig, ax = plt.subplots(nz, nx, figsize=(nx*fsize, nz*fsize))\n",
    "    ax = plt.subplot(gs1[0])\n",
    "    vis.plot_shot(hft, ax=ax, pclip=0.25, axis=False, colorbar=False)\n",
    "    \n",
    "    ax = plt.subplot(gs1[1])\n",
    "#     vis.plot_shot(iscale_vel(mp), ax=ax, pclip=1, cmap='RdBu_r', colorbar=True, axis=False, vmin=None, vmax=None)\n",
    "    vis.plot_shot(mp, ax=ax, pclip=0.25, cmap='RdBu_r', colorbar=False, axis=False, vmin=None, vmax=None)\n",
    "    \n",
    "    ax = plt.subplot(gs1[2])\n",
    "    vis.plot_shot(lft, ax=ax, pclip=0.25, axis=False, colorbar=False)\n",
    "    \n",
    "    ax = plt.subplot(gs1[3])\n",
    "    vis.plot_shot(lp, ax=ax, pclip=0.25,  axis=False, colorbar=False)\n",
    "    \n",
    "    ax = plt.subplot(gs1[4])\n",
    "    vis.plot_shot(uft, ax=ax, pclip=0.25,  axis=False, colorbar=False)\n",
    "    \n",
    "    ax = plt.subplot(gs1[5])\n",
    "    vis.plot_shot(lp_bp, ax=ax, pclip=0.02,  axis=False, colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_id = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pic_3x2(nets, net_id, '', jloader_te, imid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = nets[-1].nets[0]\n",
    "for branch_name in ['EB1', 'EB2', 'EB3']:\n",
    "    all_kernels = []\n",
    "    num_kernels_per_layer = []\n",
    "    for this_layer in getattr(net.net_encoder, branch_name):\n",
    "        kernels = this_layer.weight.cpu().detach().clone()\n",
    "        kernels = kernels - kernels.min()\n",
    "        kernels = kernels / kernels.max()\n",
    "        kernels = kernels[:, 0:1, ...].squeeze()\n",
    "        all_kernels.append(kernels)\n",
    "        num_kernels_per_layer.append(kernels.shape[0])\n",
    "    all_kernels = np.concatenate(all_kernels, 0)\n",
    "    all_kernels = np.concatenate([0.5 * np.ones((8, *all_kernels.shape[-2:])), all_kernels], 0)\n",
    "    print(f'Total {all_kernels.shape[0]} kernels in branch {branch_name}: {num_kernels_per_layer}')\n",
    "    vis.plot_data3d_slices(all_kernels, \n",
    "                           cmap='RdBu_r', \n",
    "#                            cmap='turbo',\n",
    "                           ncols=16, vmin=0.1, vmax=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class NewModel(nn.Module):\n",
    "    def __init__(self, pretrained, branch: str = 'EB1', sublayer_id: int = 3):\n",
    "        super().__init__()\n",
    "        self.branches = list(pretrained.net_encoder._modules.keys())\n",
    "        assert branch in self.branches, f\"Branch should be in {self.branches}\"\n",
    "        \n",
    "        self.layers = pretrained.net_encoder._modules.get(branch)\n",
    "        \n",
    "        self.using_norm = True\n",
    "        self.norm = F.instance_norm\n",
    "        self.act = F.elu\n",
    "        self.pads = pretrained.net_encoder._modules.get('pads')\n",
    "        if branch == 'EB1':\n",
    "            self.pad_rec = [3, 3, 3, 3, 3, 3, 6, 12, 24, 48, 3, 3, 0]\n",
    "        elif branch == 'EB2':\n",
    "            self.pad_rec = [2, 2, 2, 2, 2, 2, 4, 8, 16, 32, 2, 2, 0, 2, 2, 0]\n",
    "        elif branch == 'EB3':\n",
    "            self.pad_rec = [1, 1, 1, 1, 1, 1, 2, 4, 8, 16, 1, 1, 0, 1, 1, 0, 1, 1]\n",
    "        \n",
    "        selected_layers = []\n",
    "        for ilayer, layer in enumerate(self.layers):\n",
    "            if ilayer != sublayer_id:\n",
    "                selected_layers.append(layer)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        self.net = nn.ModuleList(selected_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.net):\n",
    "            pad_idx = self.pad_rec[i]\n",
    "            x = layer(self.pads[pad_idx](x))\n",
    "            if self.using_norm:\n",
    "                x = self.norm(x)\n",
    "            if pad_idx != 0:\n",
    "                x = self.act(x)\n",
    "        return x\n",
    "\n",
    "# model_chunk = NewModel(net, branch='EB1', sublayer_id=11)\n",
    "# from torchsummary import summary\n",
    "# summary(model_chunk, (1, 324, 376))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_activation(model, input_shape, filter_id, niter=100, lr_min=1e-2, lr_max=1):\n",
    "    random_image = np.random.uniform(-0.5, 0.5, input_shape).astype(np.float32)\n",
    "\n",
    "    processed_image = torch.tensor(random_image).unsqueeze(0).to(0)\n",
    "    processed_image.requires_grad_(True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([processed_image], lr=lr_min, betas=(0.5, 0.9), weight_decay=1e-5)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                        max_lr=lr_max,\n",
    "                                                        cycle_momentum=False,\n",
    "                                                        epochs=niter,\n",
    "                                                        steps_per_epoch=1)\n",
    "    prog_bar = range(1, niter)\n",
    "    loss_curve = []\n",
    "    for i in prog_bar:\n",
    "        optimizer.zero_grad()\n",
    "        x = processed_image\n",
    "        x = model(x)\n",
    "\n",
    "        conv_output = x[0, filter_id]\n",
    "        loss = -torch.mean(conv_output)\n",
    "        loss_curve.append(loss.sum().item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return processed_image.cpu().detach().numpy()[0,0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for layer_id in range(1, 12, 4):\n",
    "    imgs_layer = []\n",
    "    for branch in ['EB1', 'EB2', 'EB3']:\n",
    "        model_chunk = NewModel(net, branch=branch, sublayer_id=layer_id)\n",
    "        for filter_id in range(1, 8, 2):\n",
    "            print(f'Branch: {branch}\\tLayer: {layer_id}\\tFilter: {filter_id}', end='\\r')\n",
    "            imgs_layer.append(get_filter_activation(model_chunk, d[0].shape, filter_id, \n",
    "                                                    niter=500,\n",
    "                                                   lr_min=1e-5,\n",
    "                                                   lr_max =1e-1))\n",
    "    imgs.append(imgs_layer)\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(vis)\n",
    "vis.plot_nxm(imgs, transpose=False, pclip=0.125,\n",
    "             cmap='seismic', colorbar=False, \n",
    "             figsize=(2*len(imgs[0]), 2*len(imgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velocity model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_pred = './out_fwi_marm/su/'\n",
    "every_nth_shot = 20\n",
    "\n",
    "p = f'{root_pred}*.shot*'\n",
    "fnames = natsorted(glob.glob(p))\n",
    "fnames = [f for f in fnames if '.it' not in f]\n",
    "print(f'{len(fnames)} files found in {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_cgg = []\n",
    "for i, f in enumerate(fnames):\n",
    "    clear_output()\n",
    "    print(f'{i+1}/{len(fnames)}\\n{f}')\n",
    "    with segyio.su.open(f, \"r+\", endian='little', ignore_geometry=True) as dst:\n",
    "        raw = dst.trace.raw[:]\n",
    "        print(raw.shape)\n",
    "        cube_cgg.append(np.expand_dims(raw, 0))\n",
    "\n",
    "cube_cgg = np.concatenate(cube_cgg, 0)\n",
    "(cube_cgg_h, cube_cgg_l, cube_cgg_m, cube_cgg_u), _, scalers =  sd.split_hlm(cube_cgg, sd.par_default)\n",
    "loader_cgg = sd.TriLoader(cube_cgg_h, cube_cgg_l, cube_cgg_m, cube_cgg_u, np.zeros_like(cube_cgg_u), sd.par_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list `[(nshots, noffsets, ntimes)... for N networks]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = jfloader_te.__getitem__(0)\n",
    "hft = d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_id = -1\n",
    "print(net_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nets = [nets[net_id]]\n",
    "preds = [np.zeros((len(fnames), *hft.shape[-2:])) for _ in range(len(selected_nets))]\n",
    "for i, f in enumerate(fnames):\n",
    "    clear_output()\n",
    "    print(f'{i+1}/{len(fnames)}\\n{f}')\n",
    "    with segyio.su.open(f, \"r+\", endian='little', ignore_geometry=True) as dst:\n",
    "        dh_cgg = loader_cgg.__getitem__(i)[0]\n",
    "        for inet, net in enumerate(selected_nets):\n",
    "            this_pred = fns[inet][-1](net.predict(fns[inet][0](dh_cgg), 1, 0))\n",
    "            this_pred = resize(this_pred.astype(np.float32), hft.shape[-2:])\n",
    "            this_pred += 1\n",
    "            this_pred /= 2\n",
    "            this_pred *= (box_max - box_min)\n",
    "            this_pred += box_min\n",
    "            preds[inet][i, ...] = this_pred           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preds), preds[0].shape, hft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data3d_overlap(data3d, data3d_aux, ncols = 15, clip=0.05, kmix=1, figsize_coeffs=(1, 1), **kwargs):\n",
    "    nimgs = data3d.shape[0]\n",
    "    nrows = int(np.ceil(nimgs / ncols))\n",
    "    nempty = nrows * ncols - nimgs\n",
    "    _np, _no, _nt = data3d.shape\n",
    "    extended_pred = np.concatenate([data3d, np.zeros((nempty, _no, _nt))])\n",
    "    extended_pred_aux = np.concatenate([data3d_aux, np.zeros((nempty, _no, _nt))])\n",
    "    \n",
    "    the_max = np.max(extended_pred_aux)\n",
    "    the_clip = clip * the_max\n",
    "    extended_pred_aux[extended_pred_aux > the_clip] = the_clip\n",
    "    extended_pred_aux[extended_pred_aux < -the_clip] = -the_clip\n",
    "    the_max = np.max(extended_pred_aux)\n",
    "    extended_pred_mix = extended_pred + kmix * extended_pred_aux / the_max  * np.max(extended_pred)\n",
    "    \n",
    "    table = [[extended_pred_mix[ncols*irow + icol, ...] for icol in range(ncols)] for irow in range(nrows)]\n",
    "    vis.plot_nxm(table, figsize=(figsize_coeffs[0] * ncols, figsize_coeffs[1] * nrows), \n",
    "                 colorbar=True, cax_label='km/s', **kwargs)\n",
    "    \n",
    "ncols = 4\n",
    "for title, pred in zip(titles[1:], preds):\n",
    "    print(title)\n",
    "    plot_data3d_overlap(pred[::every_nth_shot, ...], cube_cgg_h[::every_nth_shot, ...],\n",
    "                       ncols=ncols, \n",
    "                        cmap='RdBu_r',\n",
    "#                         cmap='gist_ncar_r', \n",
    "                       kmix=0.075, clip=0.1,  vmin=box_min, vmax=box_max,\n",
    "                       figsize_coeffs=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hft.shape, len(fnames)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# importlib.reload(vis)\n",
    "dsrc = 9 # points between sources\n",
    "model = np.zeros((hft.shape[1] + (len(fnames)-1)*dsrc, hft.shape[-1]))\n",
    "overlaps = np.zeros_like(model)\n",
    "print(overlaps.shape)\n",
    "models = []\n",
    "for inet in range(len(selected_nets)):\n",
    "    model = np.zeros((hft.shape[-2] + dsrc * (len(fnames) - 1), hft.shape[-1]))\n",
    "    for i in range(len(fnames)):\n",
    "        model[dsrc*i:dsrc*i+hft.shape[-2], :] += preds[inet][i,...]\n",
    "        overlaps[dsrc*i:dsrc*i+hft.shape[-2], :] += 1\n",
    "    model_norm = model / overlaps\n",
    "    models.append(model_norm)\n",
    "    vis.plot_model(np.flip(model_norm.T, 0)[:, :500], title=titles[inet+1], cmap='RdBu_r', vmin=box_min, vmax=box_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_model(overlaps.T, title='Overlaps')\n",
    "print(overlaps.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
